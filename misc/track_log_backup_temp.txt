import csv
import hashlib
import json
from collections import Counter
from datetime import datetime, timedelta
from decimal import getcontext
from pathlib import Path
from typing import Any, Dict, List, Optional

from constants import (
    BASE_DIR,
    CATEGORY_WEIGHT,
    COMPLETION_THRESHOLD,
    FLOAT_PRECISION,
    MAX_DAILY_TRACKS,
    PROGRESS_WEIGHT,
    RECENCY_WEIGHT,
    TRACK_LOG_FILE,
    TRACK_STATS_FILE,
    UTC_PLUS_ONE,
)

# Set decimal precision for consistent floating-point operations
getcontext().prec = FLOAT_PRECISION


class TrackSelector:
    """
    Uses Bitcoin's deterministic k algorithm for guaranteed weekly track coverage.
    Ensures each track appears at least twice in any 7-day rolling window
    with guaranteed weekly coverage constraints.
    """

    def __init__(
        self,
        tracks_data: List[Dict[str, Any]],
        max_daily_tracks: int = MAX_DAILY_TRACKS,
    ):
        self.tracks_data = tracks_data
        self.max_daily_tracks = max_daily_tracks
        self.track_names = [t["language"] for t in tracks_data]
        self.num_tracks = len(self.track_names)

        # Weekly coverage constraints
        self.MIN_APPEARANCES_PER_WEEK = 2
        self.MAX_APPEARANCES_PER_WEEK = 4  # Prevent over-representation
        self.days_in_week = 7
        self.total_weekly_slots = self.days_in_week * self.max_daily_tracks

        # Enhanced features
        self.historical_patterns = {}
        self.adaptive_weights = {
            "progress": PROGRESS_WEIGHT,
            "recency": RECENCY_WEIGHT,
            "rotation": 0.3,
            "category": CATEGORY_WEIGHT,
        }

    def deterministic_k(
        self, message: bytes, private_key: bytes, nonce: int = 0
    ) -> int:
        """
        Bitcoin-style deterministic k generation following RFC 6979 principles.
        Ensures deterministic but cryptographically secure pseudo-randomness.
        """
        # Combine message, private key, and nonce for uniqueness
        combined = message + private_key + nonce.to_bytes(8, "big")

        # Generate deterministic k using HMAC-SHA256 chain
        k = b"\x01" * 32  # Initial k
        v = b"\x00" * 32  # Initial v

        # HMAC-SHA256 based deterministic generation (simplified RFC 6979)
        for iteration in range(5):  # Multiple rounds for better distribution
            # Update k
            h = hashlib.new("sha256")
            h.update(k + v + combined + iteration.to_bytes(1, "big"))
            k = h.digest()

            # Update v
            h = hashlib.new("sha256")
            h.update(k + v)
            v = h.digest()

        # Convert to integer in range [0, 2^64)
        return int.from_bytes(v[:8], "big")

    def calculate_priority_score(
        self,
        track_data: Dict[str, Any],
        completion_counts: Dict[str, int],
        weekly_appearances: int = 0,
        day_context: str = "",
    ) -> float:
        """
        Calculate enhanced priority score incorporating all factors.
        """
        track_name = track_data["language"]

        # Use actual completion count from cache, not from track_data
        completed = completion_counts.get(track_name, 0)
        total = track_data.get("total", 1)

        # 1. Progress factor (bell curve favoring ~50% completion)
        completion_ratio = min(completed / total, 0.99) if total > 0 else 0

        # Skip tracks that are essentially completed
        if completion_ratio >= COMPLETION_THRESHOLD:
            return -1.0  # Lowest priority

        import math

        progress_score = math.sin(completion_ratio * math.pi)

        # 2. Recency factor (prioritize tracks not touched recently)
        last_touched = track_data.get("last_touched", 0)
        max_last_touched = 30  # Reasonable maximum. Assume 30 days
        recency_score = max(0, 1.0 - (last_touched / max_last_touched))

        # 3. Weekly balance factor (discourage over-representation)
        balance_penalty = 0
        if weekly_appearances >= self.MAX_APPEARANCES_PER_WEEK:
            balance_penalty = 0.5  # Heavy penalty for over-representation
        elif weekly_appearances >= self.MIN_APPEARANCES_PER_WEEK:
            balance_penalty = 0.1  # Light penalty once minimum is met

        balance_score = 1.0 - balance_penalty

        # 4. Category diversity factor (can be enhanced with more context)
        category_score = 0.5  # Neutral for now, could incorporate category balancing

        # Combine factors with weights
        final_score = (
            progress_score * PROGRESS_WEIGHT
            + recency_score * RECENCY_WEIGHT
            + balance_score * 0.3  # Weekly balance weight
            + category_score * CATEGORY_WEIGHT
        )

        return max(0, final_score)  # Ensure non-negative

    def generate_weekly_schedule(
        self, week_start: datetime, completion_counts: Dict[str, int]
    ) -> List[List[str]]:
        """
        Generate a complete weekly schedule ensuring each track appears at least twice with coverage constraints.
        Uses Bitcoin-style deterministic generation.
        """
        week_str = week_start.strftime("%Y-W%U")
        private_key = hashlib.sha256(f"exercism-week-{week_str}".encode()).digest()

        # Initialize daily schedules
        weekly_schedule = [[] for _ in range(7)]
        track_appearances = Counter()

        # Filter viable tracks (not completed)
        viable_tracks = []
        for track_data in self.tracks_data:
            track_name = track_data["language"]
            completed = completion_counts.get(track_name, 0)
            total = track_data.get("total", 1)
            completion_ratio = completed / total if total > 0 else 0

            if completion_ratio < COMPLETION_THRESHOLD:
                viable_tracks.append(track_data)

        if not viable_tracks:
            return weekly_schedule

        # Phase 1: Ensure minimum appearances for all viable tracks
        # Sort by priority to place highest priority tracks first
        track_priorities = []
        for track_data in viable_tracks:
            priority = self.calculate_priority_score(
                track_data, completion_counts, 0, "initial"
            )
            if priority > 0:
                track_priorities.append((priority, track_data))

        track_priorities.sort(reverse=True)

        # Distribute minimum appearances
        for _, track_data in track_priorities:
            track_name = track_data["language"]

            # Ensure each track gets minimum appearances
            while track_appearances[track_name] < self.MIN_APPEARANCES_PER_WEEK:
                best_day = self.select_best_day_for_track(
                    track_name,
                    weekly_schedule,
                    week_str,
                    private_key,
                    track_appearances,
                )

                if best_day is not None:
                    weekly_schedule[best_day].append(track_name)  # type: ignore
                    track_appearances[track_name] += 1
                else:
                    break  # No more slots available

        # Phase 2: Fill remaining slots with best available tracks
        for day_idx in range(7):
            while len(weekly_schedule[day_idx]) < self.max_daily_tracks:
                best_track = self.select_best_track_for_slot(
                    day_idx,
                    weekly_schedule,
                    track_appearances,
                    viable_tracks,
                    completion_counts,
                    week_str,
                    private_key,
                )

                if best_track:
                    weekly_schedule[day_idx].append(best_track)
                    track_appearances[best_track] += 1
                else:
                    break  # No more suitable tracks

        return weekly_schedule

    def select_best_day_for_track(
        self,
        track_name: str,
        current_schedule: List[List[str]],
        week_str: str,
        private_key: bytes,
        track_appearances: Counter,
    ) -> Optional[str]:
        """
        Select the best day for placing a specific track using deterministic logic.
        """
        available_days = [
            day
            for day in range(7)
            if len(current_schedule[day]) < self.max_daily_tracks
            and track_name not in current_schedule[day]
        ]

        if not available_days:
            return None

        # Use deterministic k to select among available days
        message = f"{week_str}-{track_name}-day-selection-{track_appearances[track_name]}".encode()
        k = self.deterministic_k(message, private_key)

        # Weight days by available slots (prefer less crowded days)
        day_weights = []
        for day in available_days:
            available_slots = self.max_daily_tracks - len(current_schedule[day])
            # Add deterministic variance
            weight = available_slots * 1000 + (k + day) % 1000
            day_weights.append((weight, day))

        day_weights.sort(reverse=True)
        return day_weights[0][1]

    def select_best_track_for_slot(
        self,
        day_idx: int,
        current_schedule: List[List[str]],
        track_appearances: Counter,
        tracks_data: List[Dict],
        completion_counts: Dict[str, int],
        week_str: str,
        private_key: bytes,
    ) -> Optional[str]:
        """
        Select the best track for a specific slot using deterministic scoring
        """
        # Find tracks not already scheduled for this day
        available_tracks = [
            track_data
            for track_data in tracks_data
            if track_data["language"] not in current_schedule[day_idx]
            and track_appearances[track_data["language"]]
            < self.MAX_APPEARANCES_PER_WEEK
        ]

        if not available_tracks:
            return None

        # Score each available track
        track_scores = []
        for track_data in available_tracks:
            track_name = track_data["language"]
            current_appearances = track_appearances[track_name]

            # Base priority score
            base_score = self.calculate_priority_score(
                track_data, completion_counts, current_appearances, f"day-{day_idx}"
            )

            if base_score <= 0:
                continue  # Skip completed or invalid tracks

            # Add deterministic variance for tie-breaking
            message = f"{week_str}-{track_name}-day-{day_idx}-slot".encode()
            k = self.deterministic_k(message, private_key)
            variance = (k % 1000) / 10000  # Small random component (0-0.1)

            final_score = base_score + variance
            track_scores.append((final_score, track_name))

        if not track_scores:
            return None

        # Return highest scoring track
        track_scores.sort(reverse=True)
        return track_scores[0][1]

    def get_daily_tracks(
        self, target_date: datetime, completion_counts: Dict[str, int]
    ) -> List[str]:
        """
        Get tracks for a specific date using the weekly schedule.
        """
        # Find the Monday of the week containing target_date
        days_since_monday = target_date.weekday()
        week_start = target_date - timedelta(days=days_since_monday)

        # Generate weekly schedule
        weekly_schedule = self.generate_weekly_schedule(week_start, completion_counts)

        # Return tracks for target day
        day_of_week = target_date.weekday()  # 0 = Monday
        return (
            weekly_schedule[day_of_week] if day_of_week < len(weekly_schedule) else []
        )

    def validate_weekly_coverage(
        self, weekly_schedule: List[List[str]]
    ) -> Dict[str, Any]:
        """
        Validate that weekly schedule meets coverage requirements.
        """
        track_counts = Counter()
        for day_tracks in weekly_schedule:
            for track in day_tracks:
                track_counts[track] += 1

        # Analysis
        total_unique_tracks = len(set(self.track_names))
        tracks_with_min_coverage = len(
            [
                t
                for t in self.track_names
                if track_counts[t] >= self.MIN_APPEARANCES_PER_WEEK
            ]
        )

        missing_tracks = [
            t
            for t in self.track_names
            if track_counts[t] < self.MIN_APPEARANCES_PER_WEEK
        ]

        over_represented = [
            t
            for t in self.track_names
            if track_counts[t] > self.MAX_APPEARANCES_PER_WEEK
        ]

        return {
            "total_slots_filled": sum(len(day) for day in weekly_schedule),
            "total_possible_slots": self.days_in_week * self.max_daily_tracks,
            "unique_tracks_scheduled": len(track_counts),
            "total_possible_tracks": total_unique_tracks,
            "tracks_meeting_minimum": tracks_with_min_coverage,
            "coverage_percentage": (tracks_with_min_coverage / total_unique_tracks)
            * 100
            if total_unique_tracks > 0
            else 0,
            "missing_tracks": missing_tracks,
            "over_represented_tracks": over_represented,
            "track_distribution": dict(track_counts),
            "average_appearances": sum(track_counts.values()) / len(track_counts)
            if track_counts
            else 0,
        }


def count_completed_exercises(base_url: Path, track: str) -> int:
    """Count completed exercises for a given track (non-hidden subdirs)."""
    track_path = base_url / track
    if not track_path.exists() or not track_path.is_dir():
        return 0
    try:
        return sum(
            1 for d in track_path.iterdir() if d.is_dir() and not d.name.startswith(".")
        )
    except (PermissionError, FileNotFoundError) as e:
        print(f"Error accessing {track_path}: {e}")
        return 0


def get_cached_completion_counts(today_str: str) -> Dict[str, int]:
    """Get cached completion counts for today, or generate new ones if not cached."""
    cache_file = BASE_DIR / "database" / f"completion_cache_{today_str}.json"

    if cache_file.exists():
        try:
            with open(cache_file, "r") as f:
                cached_data = json.load(f)
                print(f"Using cached completion counts from {cache_file}")
                return cached_data
        except (json.JSONDecodeError, FileNotFoundError) as e:
            print(f"Error reading cache file: {e}")

    # Cache doesn't exist or is invalid, generate new counts
    print("Generating fresh completion counts...")
    languages = load_language_metadata(TRACK_STATS_FILE)
    completion_counts = {}

    for lang_data in languages:
        track = lang_data["language"]
        completed = count_completed_exercises(BASE_DIR, track)
        completion_counts[track] = completed
        print(f"  {track}: {completed} exercises completed")

    # Save to cache
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_file, "w") as f:
        json.dump(completion_counts, f, indent=2)

    print(f"Completion counts cached to {cache_file}")
    return completion_counts


def load_language_metadata(track_stats_file: Path) -> List[Dict[str, Any]]:
    """Load language metadata from JSON file."""
    try:
        with open(track_stats_file, "r") as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"Error loading metadata: {e}")
        return []


def log_daily_selection(
    track_log_file: Path,
    today_str: str,
    selected_tracks: List[str],
    weekly_validation: Dict,
):
    """Log today's track selection with weekly coverage info to CSV file."""
    try:
        with open(track_log_file, "a", newline="") as f:
            writer = csv.writer(f)
            # Main entry
            writer.writerow([today_str, ",".join(selected_tracks)])

    except Exception as e:
        print(f"Error logging selection: {e}")


def ensure_log_file_exists(track_log_file: Path) -> None:
    """Ensure the log file exists with proper header."""
    if not track_log_file.exists():
        track_log_file.parent.mkdir(parents=True, exist_ok=True)
        with open(track_log_file, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(["date", "tracks"])


def is_already_logged(track_log_file: Path, today_str: str) -> bool:
    """Check if today's selection is already logged."""
    if not track_log_file.exists():
        return False

    try:
        with open(track_log_file, "r") as f:
            existing_lines = f.read().splitlines()
            existing_dates = {
                line.split(",")[0] for line in existing_lines[1:] if "," in line
            }
            return today_str in existing_dates
    except Exception as e:
        print(f"Error checking log file: {e}")
        return False


def main() -> None:
    """Main function with improved track selection."""
    # Load track data
    languages = load_language_metadata(TRACK_STATS_FILE)
    if not languages:
        print("No language metadata found. Exiting.")
        return

    # Get today's date and completion counts
    today = datetime.now(UTC_PLUS_ONE)
    today_str = today.strftime("%Y-%m-%d")
    completion_counts = get_cached_completion_counts(today_str)

    if not completion_counts:
        print("No completion counts available. Exiting.")
        return

    # Initialize track selector
    selector = TrackSelector(languages, MAX_DAILY_TRACKS)

    # Get daily tracks using weekly planning
    selected_tracks = selector.get_daily_tracks(today, completion_counts)

    # Generate weekly schedule for analysis
    days_since_monday = today.weekday()
    week_start = today - timedelta(days=days_since_monday)
    weekly_schedule = selector.generate_weekly_schedule(week_start, completion_counts)

    # Validate coverage
    validation = selector.validate_weekly_coverage(weekly_schedule)

    # Output results
    print(f"\nSelected tracks for {today_str}:")
    for track in selected_tracks:
        track_data = next((t for t in languages if t["language"] == track), None)
        if track_data:
            completed = completion_counts.get(track, 0)
            total = track_data["total"]
            completion_ratio = completed / total if total > 0 else 0
            print(
                f"- {track} ({track_data['category']}): "
                f"{completed}/{total} exercises "
                f"({completion_ratio:.0%})"
            )

    # Log the selection (if not already logged)
    ensure_log_file_exists(TRACK_LOG_FILE)
    if not is_already_logged(TRACK_LOG_FILE, today_str):
        log_daily_selection(TRACK_LOG_FILE, today_str, selected_tracks, validation)
        print(f"\nLogged selection to {TRACK_LOG_FILE}")
    else:
        print(f"\nSelection already logged for {today_str}")

    print("\nWeekly Coverage Analysis:")
    print(
        f"Coverage: {validation['coverage_percentage']:.1f}% "
        f"({validation['tracks_meeting_minimum']}/{validation['total_possible_tracks']} tracks)"
    )
    print(f"Missing tracks: {validation['missing_tracks']}")
    if validation["over_represented_tracks"]:
        print(f"Over-represented: {validation['over_represented_tracks']}")

    # Show weekly schedule for debugging
    print(f"\nFull weekly schedule starting {week_start.strftime('%Y-%m-%d')}:")
    days = [
        "Monday",
        "Tuesday",
        "Wednesday",
        "Thursday",
        "Friday",
        "Saturday",
        "Sunday",
    ]
    for i, day_tracks in enumerate(weekly_schedule):
        print(f"  {days[i]}: {day_tracks}")


if __name__ == "__main__":
    main()
